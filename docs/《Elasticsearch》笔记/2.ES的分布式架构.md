---
title: ES分布式架构
date: 2025-05-21 19:54:18
permalink: /pages/5113b6/
categories:
  - 《Elasticsearch》笔记
tags:
  - ES分布式架构
author: 
  name: Tavio
  link: https://github.com/tavio-zhang
---

在大数据时代，如何高效存储、快速检索海量数据成为关键挑战。Elasticsearch（简称ES）凭借其分布式特性，成为解决这一问题的主流方案。

## 一、为什么需要分布式架构？

ES作为开源的分布式搜索引擎，其设计初衷就是为了解决单机模式的三大瓶颈：
- **存储瓶颈**：单机磁盘容量有限，无法承载TB级甚至PB级数据；
- **性能瓶颈**：单节点处理能力有限，高并发查询时响应延迟剧增；
- **可用性瓶颈**：单机故障会导致服务中断，数据存在丢失风险。

分布式架构通过"分而治之"的思想，将数据分散到多个节点，同时通过冗余备份保障高可用，完美解决了上述问题。

## 二、ES分布式架构的四大核心概念

ES的分布式能力基于四个核心概念构建，它们相互关联、层层支撑，共同构成了分布式体系的基石。

### 2.1 节点：ES集群的物理服务器
节点是ES集群的最小硬件单元，本质是一台运行了ES软件的服务器。所有的数据存储、查询计算都在节点上完成，多个节点组成了ES集群。

如果把ES看作一个图书馆，每个楼层就是一个节点，不同楼层之间有通信机制。同时节点间会按功能分工，避免单一节点压力过大宕机。

| 节点类型 | 核心职责 | 配置方式（elasticsearch.yml） |
| - | - | - |
| 主节点（Master） | 管理集群元数据（如索引创建、分片分配）、维护集群状态（节点加入/离开） | `node.master: true`（默认true） |
| 数据节点（Data） | 存储分片、处理读写请求、执行数据聚合计算 | `node.data: true`（默认true） |
| 协调节点（Coordinating） | 接收请求、分发任务、汇总结果 | 默认为所有节点（仅当`node.master: false`且`node.data: false`时为专用协调节点） |
| 预处理节点（Ingest） | 数据写入前进行预处理（如字段转换、过滤） | `node.ingest: true`（默认true） |

> 注意：一个节点可同时承担多种角色（如默认节点既是主节点候选者，也是数据节点和协调节点），但生产环境建议分离角色以避免资源竞争。

### 2.2 索引：数据的逻辑集合
索引是同类文档的**逻辑容器**，类似关系型数据库中的"数据库"概念，但包含更丰富的含义：
- 定义数据结构规则（映射，Mapping）；
- 管理分片与副本的分配策略；
- 作为客户端操作的入口（如查询、写入均通过索引名指定）。

#### 索引的关键特性
- **命名规范**：必须小写，不能包含空格、斜杠等特殊字符（推荐使用`product-logs`这类下划线/短横线分隔的命名）；
- **映射（Mapping）**：定义文档字段的类型（如文本、数字、日期）、分词器等规则，类似数据库的"表结构"；
- **生命周期**：可通过ILM（索引生命周期管理）自动执行滚动、归档、删除等操作，适用于时序数据（如日志）。

### 2.3 分片：数据的水平拆分单元
当索引数据量过大时，单节点无法承载，此时需要通过**分片**将索引拆分为多个独立的子单元。每个分片是一个完整的Lucene实例（ES底层搜索引擎），可独立处理读写请求。

#### 分片的核心作用
1. **突破存储限制**：将1亿条数据拆分为5个分片，每个分片仅需存储2000万条；
2. **并行计算加速**：查询时多个分片同时处理，结果汇总后返回，效率随分片数线性提升（在合理范围内）。

#### 分片的类型与特性
| 分片类型 | 核心特性 |
| - | - |
| 主分片（Primary Shard） | 数据写入的原始分片，创建索引时指定数量（默认5个），**创建后不可修改**（修改需重建索引） |
| 副分片（Replica Shard） | 主分片的冗余副本，可动态调整数量（默认1个），不参与数据写入但可处理读请求 |

> 为什么主分片数量不可修改？  
> 文档存储的分片由哈希算法决定：`shard = hash(document_id) % 主分片数`。若修改主分片数，哈希结果会变化，导致所有文档的存储位置失效。

### 2.4 副本：高可用与性能优化的保障
副本是主分片的**冗余拷贝**，其核心价值体现在两方面：
- **高可用**：当主分片所在节点故障时，副本会被自动升级为主分片，避免数据丢失；
- **性能优化**：副本可分担读请求压力（如查询、聚合），提高集群读吞吐量。

#### 副本的分配规则
- 主分片与副本分片**不会存储在同一节点**（避免单点故障）；
- 多个副本会均匀分布在不同节点（如2个副本会分配到2个不同节点）。

## 三、核心概念的关联关系
四个概念的层级关系可概括为：**集群由节点组成，索引由分片组成，分片有副本保障高可用**。

用"图书馆"模型类比：
- 集群 = 整个图书馆；
- 节点 = 图书馆的楼层（每个楼层有独立的存储空间和工作人员）；
- 索引 = 某类书籍的专区（如"计算机科学区"）；
- 主分片 = 专区内的书架（原始书籍存放处）；
- 副本 = 书架的备份（复本书籍，放在其他楼层，供读者查阅或应急）；
- 协调节点 = 图书馆前台（接收读者请求，协调各楼层找书，汇总结果）。

<img src="/img/es-cluster-example.png">  
<img src="/img/es-cluster-demo.png">
*图：ES集群中节点、索引、分片、副本的分布关系（主分片与副本分散在不同节点，保障高可用）*

**记住 4 个核心结论**
>  节点是 “硬件载体”：运行 ES 的服务器，是所有数据和计算的基础；  
>  索引是 “逻辑集合”：管理同类数据，定义数据规则，是客户端操作的入口；  
>  分片是 “水平拆分”：解决单节点存储和查询瓶颈，数量创建后不可改；  
>  副本是 “冗余备份”：保障高可用，数量可动态调整。

## 四、Docker Compose 搭建本地ES集群实战
下面通过Docker Compose快速搭建一个包含3个节点的ES集群，并集成Kibana可视化工具。

### 4.1 集群架构设计
- 3个节点（es-node1、es-node2、es-node3），均作为主节点候选者和数据节点；
- 每个节点分配512M堆内存（开发环境配置，生产环境需调大）；
- Kibana用于集群监控与操作界面。

### 4.2 配置文件（docker-compose.yml）
```
version: '3.8'  # 明确Compose版本

services:
  # 节点1：主节点候选者+数据节点
  es-node1:
    image: elasticsearch:7.17.10  # 使用官方镜像，版本需统一
    container_name: es-node1
    environment:
      - node.name=es-node1  # 节点唯一名称
      - cluster.name=es-learning-cluster  # 集群名称（所有节点必须一致）
      - discovery.seed_hosts=es-node2,es-node3  # 集群发现列表（其他节点的主机名）
      - cluster.initial_master_nodes=es-node1,es-node2,es-node3  # 初始主节点候选列表（避免脑裂）
      - bootstrap.memory_lock=true  # 锁定内存，防止ES内存被系统swap至磁盘（影响性能）
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"  # 堆内存大小（生产环境建议设为物理内存的50%，不超过32G）
      - xpack.security.enabled=false  # 关闭安全认证（开发环境简化配置）
      - xpack.monitoring.collection.enabled=true  # 开启监控数据收集（供Kibana展示）
    ulimits:
      memlock:
        soft: -1  # 无限制
        hard: -1
    volumes:
      - es-data1:/usr/share/elasticsearch/data  # 数据持久化卷（避免容器重启数据丢失）
    ports:
      - "9200:9200"  # HTTP端口（供客户端访问）
      - "9300:9300"  # TCP端口（集群内部通信）
    networks:
      - es-network  # 集群专用网络（隔离其他服务）

  # 节点2：配置与节点1一致，仅名称和端口不同
  es-node2:
    image: elasticsearch:7.17.10
    container_name: es-node2
    environment:
      - node.name=es-node2
      - cluster.name=es-learning-cluster
      - discovery.seed_hosts=es-node1,es-node3
      - cluster.initial_master_nodes=es-node1,es-node2,es-node3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
      - xpack.monitoring.collection.enabled=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es-data2:/usr/share/elasticsearch/data
    ports:
      - "9201:9200"
      - "9301:9300"
    networks:
      - es-network

  # 节点3：配置与节点1一致，仅名称和端口不同
  es-node3:
    image: elasticsearch:7.17.10
    container_name: es-node3
    environment:
      - node.name=es-node3
      - cluster.name=es-learning-cluster
      - discovery.seed_hosts=es-node1,es-node2
      - cluster.initial_master_nodes=es-node1,es-node2,es-node3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
      - xpack.monitoring.collection.enabled=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es-data3:/usr/share/elasticsearch/data
    ports:
      - "9202:9200"
      - "9302:9300"
    networks:
      - es-network

  # Kibana：ES可视化工具
  kibana:
    image: kibana:7.17.10  # 版本必须与ES一致
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=["http://es-node1:9200","http://es-node2:9200","http://es-node3:9200"]  # 连接所有ES节点
      - KIBANA_DEFAULTAPPID=home  # 默认启动页
      - I18N_LOCALE=zh-CN  # 中文显示
    ports:
      - "5601:5601"  # Kibana Web端口
    depends_on:
      - es-node1
      - es-node2
      - es-node3  # 确保ES启动后再启动Kibana
    networks:
      - es-network

# 定义持久化卷（数据不会随容器删除而丢失）
volumes:
  es-data1:
  es-data2:
  es-data3:

# 定义集群网络
networks:
  es-network:
    driver: bridge  # 桥接模式（适合本地开发）
```

### 4.3 启动与验证集群
1. **启动集群**：在配置文件目录执行以下命令
```
docker-compose up -d  # -d表示后台运行
```

2. **验证集群状态**：通过ES API查看集群健康度
```
curl http://localhost:9200/_cluster/health

//成功返回应为：
{
  "cluster_name": "es-learning-cluster",
  "status": "green",  # green表示所有主分片和副本均正常
  "timed_out": false,
  "number_of_nodes": 3,  # 3个节点正常运行
  "number_of_data_nodes": 3,
  "active_primary_shards": 0,  # 暂无索引，所以分片数为0
  "active_shards": 0,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 0,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 100.0
}
```

3. **访问Kibana**：打开浏览器访问 `http://localhost:5601`，进入Kibana界面即可可视化管理集群。

## 五、核心结论与最佳实践
1. **节点规划**：生产环境建议分离角色（如1-3个专用主节点，多个数据节点，专用协调节点）；
2. **分片策略**：主分片数量根据数据量预估（每个分片建议10-50GB），副本数根据高可用需求设置（至少1个）；
3. **性能优化**：副本可提升读性能，但会增加存储开销和写入延迟（需平衡读写需求）；
4. **扩展性**：通过新增节点可自动扩展集群容量，ES会自动重新分配分片。